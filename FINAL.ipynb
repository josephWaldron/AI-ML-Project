{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tensorflow Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for all tensorflow models\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def plot_accuracy_loss(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 16\n",
    "PATH = \"../data/CUB_200_2011\"\n",
    "labels = pd.read_csv(os.path.join(PATH, \"image_class_labels.txt\"), sep=\" \", header=None, names=[\"img_id\", \"label\"])\n",
    "images = pd.read_csv(os.path.join(PATH, \"images.txt\"), sep=\" \", header=None, names=[\"img_id\", \"filepath\"])\n",
    "classes = pd.read_csv(os.path.join(PATH, \"classes.txt\"), sep=\" \", header=None, names=[\"label\", \"category\"])\n",
    "num_classes = len(classes)\n",
    "df = pd.merge(images, labels, on=\"img_id\")\n",
    "df = pd.merge(df, classes, on=\"label\")\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "# Initialize transformations for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.5, 1.5], # Mimics ColorJitter's brightness adjustment\n",
    "    channel_shift_range=0.2, # Partially mimics ColorJitter's hue adjustment\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the dataset and create generators\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    directory=os.path.join(PATH, \"images\"),\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    dataframe=train_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"category\",\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.5, 1.5], # Mimics ColorJitter's brightness adjustment\n",
    "    channel_shift_range=0.2, # Partially mimics ColorJitter's hue adjustment\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    directory=os.path.join(PATH, \"images\"),\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    dataframe=test_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"category\",\n",
    ")\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.5, 1.5], # Mimics ColorJitter's brightness adjustment\n",
    "    channel_shift_range=0.2, # Partially mimics ColorJitter's hue adjustment\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    directory=os.path.join(PATH, \"images\"),\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    dataframe=val_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"category\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16 early stopping\n",
    "early_stopping = EarlyStopping(patience=20, restore_best_weights=True, min_delta=0, monitor='val_accuracy')\n",
    "num_epochs = 200\n",
    "leanring_rate = 0.0001\n",
    "\n",
    "vgg16Model = VGG16(weights='imagenet', include_top=False)\n",
    "x = GlobalAveragePooling2D()(vgg16Model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=vgg16Model.input, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer=Adam(lr=leanring_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_generator, epochs=num_epochs, validation_data=val_generator ,callbacks=[early_stopping])\n",
    "plot_accuracy_loss(history)\n",
    "\n",
    "model.save('../models/vgg16_EARLY.h5')\n",
    "model = tf.keras.models.load_model('../models/vgg16_EARLY.h5')\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16 200 epochs\n",
    "num_epochs = 200\n",
    "leanring_rate = 0.0001\n",
    "\n",
    "vgg16Model = VGG16(weights='imagenet', include_top=False)\n",
    "x = GlobalAveragePooling2D()(vgg16Model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=vgg16Model.input, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer=Adam(lr=leanring_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_generator, epochs=num_epochs, validation_data=val_generator)\n",
    "plot_accuracy_loss(history)\n",
    "\n",
    "model.save('../models/vgg16.h5')\n",
    "model = tf.keras.models.load_model('../models/vgg16.h5')\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet 50 early stopping\n",
    "early_stopping = EarlyStopping(patience=20, restore_best_weights=True, min_delta=0, monitor='val_accuracy')\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0001\n",
    "# Define the model\n",
    "resnetModel = ResNet50(weights='imagenet', include_top=False)\n",
    "x = GlobalAveragePooling2D()(resnetModel.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=resnetModel.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=num_epochs, validation_data=val_generator,callbacks=[early_stopping])\n",
    "plot_accuracy_loss(history)\n",
    "\n",
    "model.save('../models/ResNet50_EARLY.h5')\n",
    "model = tf.keras.models.load_model('../models/ResNet50_EARLY.h5')\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet 50 200 epochs\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0001\n",
    "# Define the model\n",
    "resnetModel = ResNet50(weights='imagenet', include_top=False)\n",
    "x = GlobalAveragePooling2D()(resnetModel.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=resnetModel.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=num_epochs, validation_data=val_generator)\n",
    "plot_accuracy_loss(history)\n",
    "\n",
    "model.save('../models/ResNet50.h5')\n",
    "model = tf.keras.models.load_model('../models/ResNet50.h5')\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generators for 299x299 images\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.5, 1.5], # Mimics ColorJitter's brightness adjustment\n",
    "    channel_shift_range=0.2, # Partially mimics ColorJitter's hue adjustment\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load the dataset and create generators\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    directory=os.path.join(PATH, \"images\"),\n",
    "    target_size=(299, 299),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    dataframe=train_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"category\",\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.5, 1.5], # Mimics ColorJitter's brightness adjustment\n",
    "    channel_shift_range=0.2, # Partially mimics ColorJitter's hue adjustment\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    directory=os.path.join(PATH, \"images\"),\n",
    "    target_size=(299, 299),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    dataframe=test_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"category\",\n",
    ")\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.5, 1.5], # Mimics ColorJitter's brightness adjustment\n",
    "    channel_shift_range=0.2, # Partially mimics ColorJitter's hue adjustment\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    directory=os.path.join(PATH, \"images\"),\n",
    "    target_size=(299, 299),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    dataframe=val_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"category\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#InceptionV3 early stopping\n",
    "early_stopping = EarlyStopping(patience=20, restore_best_weights=True, min_delta=0, monitor='val_accuracy')\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0001\n",
    "# Define the model\n",
    "inceptionModel = InceptionV3(weights='imagenet', include_top=False)\n",
    "x = GlobalAveragePooling2D()(inceptionModel.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=inceptionModel.input, outputs=predictions)\n",
    "model.compile(optimizer=Adam(lr=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=num_epochs,validation_data=val_generator, callbacks=[early_stopping])\n",
    "plot_accuracy_loss(history)\n",
    "model.save('../models/InceptionV3_EARLY.h5')\n",
    "\n",
    "model = tf.keras.models.load_model('../models/InceptionV3_EARLY.h5')\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incepionV3 200 epochs\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0001\n",
    "# Define the model\n",
    "inceptionModel = InceptionV3(weights='imagenet', include_top=False)\n",
    "x = GlobalAveragePooling2D()(inceptionModel.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=inceptionModel.input, outputs=predictions)\n",
    "model.compile(optimizer=Adam(lr=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=num_epochs,validation_data=val_generator)\n",
    "plot_accuracy_loss(history)\n",
    "model.save('../models/InceptionV3.h5')\n",
    "\n",
    "model = tf.keras.models.load_model('../models/InceptionV3.h5')\n",
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for all pytorch models\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "run_on_local = 1\n",
    "\n",
    "if run_on_local == 1:\n",
    "    working_directory = r'C:\\Users\\ODIN\\Desktop\\ml_final'\n",
    "    os.chdir(working_directory)\n",
    "    print(\"Current Working Directory: \", os.getcwd())\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !mkdir /content/data\n",
    "    !tar -xvzf /content/drive/MyDrive/ML_final/CUB_200_2011.tgz -C /content/data\n",
    "\n",
    "# if run_on_local == 1:\n",
    "#     %cd C:\\Users\\ODIN\\Desktop\\ml_final\\\n",
    "\n",
    "# Create Training/Val Loss Graphs\n",
    "def create_graph(epoch_train_loss, epoch_val_loss):\n",
    "    plt.plot(epoch_train_loss, label='Training Loss')\n",
    "    plt.plot(epoch_val_loss, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    return plt.show()\n",
    "\n",
    "# Early Stopping\n",
    "# https://www.youtube.com/watch?v=lMMlbmfvKDQ\n",
    "# https://github.com/jeffheaton/app_deep_learning/blob/main/t81_558_class_03_4_early_stop.ipynb\n",
    "\n",
    "import copy\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif self.best_loss - val_loss >= self.min_delta:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "#check aviable weights\n",
    "import torch\n",
    "\n",
    "weight_enum = torch.hub.load(\"pytorch/vision\", \"get_model_weights\", name=\"inception_v3\")\n",
    "print([weight for weight in weight_enum])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=45),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "#dataset = torchvision.datasets.ImageFolder(root='/content/data/CUB_200_2011/images', transform=transform\n",
    "dataset = torchvision.datasets.ImageFolder(root='./content/data/CUB_200_2011/images', transform=transform)\n",
    "# Split to 80%/10%/10%\n",
    "\n",
    "train_split = int(len(dataset) * .8)\n",
    "val_split = int(len(dataset) * .1)\n",
    "test_split = int(len(dataset) * .1)\n",
    "\n",
    "#Make sure splits match 1:1 with dataset\n",
    "total = train_split + val_split + test_split\n",
    "if total < len(dataset):\n",
    "    train_split = train_split + (len(dataset) - total)\n",
    "\n",
    "\n",
    "\n",
    "print(dataset, train_split, val_split, test_split, train_split + val_split + test_split )\n",
    "train, val, test = torch.utils.data.random_split(dataset, [train_split, val_split, test_split])\n",
    "\n",
    "num_workers=2\n",
    "batch_size=16\n",
    "\n",
    "trainLoader = DataLoader(train , batch_size=batch_size,\n",
    "                                           num_workers=num_workers,  shuffle=True)\n",
    "valLoader = DataLoader(val, batch_size=batch_size,\n",
    "                                          num_workers=num_workers )\n",
    "testLoader = DataLoader(test, batch_size=batch_size,\n",
    "                                          num_workers=num_workers)\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "weightz = models.get_model_weights(\"vgg16\")\n",
    "weightz = weightz.DEFAULT\n",
    "print(weightz)\n",
    "\n",
    "vgg16 = models.vgg16(weights=weightz)\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "print(\"classes:\", num_classes)\n",
    "\n",
    "vgg16.classifier[6] = torch.nn.Linear(vgg16.classifier[6].in_features, num_classes)\n",
    "\n",
    "# Create an object of the model\n",
    "vgg16 = vgg16.to(\"cuda\")\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.RMSprop(vgg16.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "import tqdm\n",
    "\n",
    "es = EarlyStopping(patience=20, min_delta=0, restore_best_weights=True)\n",
    "\n",
    "\n",
    "epoch_train_loss = []\n",
    "epoch_val_loss = []\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = []\n",
    "    vgg16.train()\n",
    "    pbar = tqdm.tqdm(trainLoader, total=len(trainLoader))\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vgg16(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        pbar.set_description(f\"Epoch: [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = np.mean(train_loss)\n",
    "    epoch_train_loss.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    vgg16.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valLoader:\n",
    "            images = images.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "            outputs = vgg16(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    avg_val_loss = np.mean(val_loss)\n",
    "    epoch_val_loss.append(avg_val_loss)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Val Loss: {np.mean(avg_val_loss):.4f}')\n",
    "\n",
    "    # Check for ES\n",
    "    if es(vgg16, avg_val_loss):\n",
    "        pbar.set_description(f\"Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, {es.status}\")\n",
    "        break\n",
    "    else:\n",
    "        pbar.set_description(f\"Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "create_graph(epoch_train_loss, epoch_val_loss)\n",
    "#Test the model\n",
    "y_test = []\n",
    "y_test_predict = []\n",
    "\n",
    "vgg16.eval()\n",
    "for images, labels in testLoader:\n",
    "    images = images.to(\"cuda\")\n",
    "    labels = labels.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = vgg16(images).cpu()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_test_predict.extend(predicted.numpy())\n",
    "        y_test.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_test_predict))\n",
    "\n",
    "if run_on_local == 1:\n",
    "    save_path = './'\n",
    "else:\n",
    "    save_path = '/content/drive/MyDrive/ML_final/'\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': vgg16.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            }, save_path + '200_vgg16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet50\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=45),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "#dataset = torchvision.datasets.ImageFolder(root='/content/data/CUB_200_2011/images', transform=transform\n",
    "dataset = torchvision.datasets.ImageFolder(root='./content/data/CUB_200_2011/images', transform=transform)\n",
    "# Split to 80%/10%/10%\n",
    "\n",
    "train_split = int(len(dataset) * .8)\n",
    "val_split = int(len(dataset) * .1)\n",
    "test_split = int(len(dataset) * .1)\n",
    "\n",
    "#Make sure splits match 1:1 with dataset\n",
    "total = train_split + val_split + test_split\n",
    "if total < len(dataset):\n",
    "    train_split = train_split + (len(dataset) - total)\n",
    "\n",
    "\n",
    "\n",
    "print(dataset, train_split, val_split, test_split, train_split + val_split + test_split )\n",
    "train, val, test = torch.utils.data.random_split(dataset, [train_split, val_split, test_split])\n",
    "\n",
    "num_workers=2\n",
    "batch_size=16\n",
    "\n",
    "trainLoader = DataLoader(train , batch_size=batch_size,\n",
    "                                           num_workers=num_workers,  shuffle=True)\n",
    "valLoader = DataLoader(val, batch_size=batch_size,\n",
    "                                          num_workers=num_workers )\n",
    "testLoader = DataLoader(test, batch_size=batch_size,\n",
    "                                          num_workers=num_workers)\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "weightz = models.get_model_weights(\"resnet50\")\n",
    "weightz = weightz.DEFAULT\n",
    "print(weightz)\n",
    "\n",
    "resnet50 = models.resnet50(weights=weightz)\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "print(\"classes:\", num_classes)\n",
    "\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_features = resnet50.fc.in_features\n",
    "resnet50.fc = torch.nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Create an object of the model\n",
    "resnet50 = resnet50.to(\"cuda\")\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.RMSprop(resnet50.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "import tqdm\n",
    "\n",
    "es = EarlyStopping(patience=20, min_delta=0, restore_best_weights=True)\n",
    "\n",
    "\n",
    "epoch_train_loss = []\n",
    "epoch_val_loss = []\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = []\n",
    "    resnet50.train()\n",
    "    pbar = tqdm.tqdm(trainLoader, total=len(trainLoader))\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet50(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        pbar.set_description(f\"Epoch: [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = np.mean(train_loss)\n",
    "    epoch_train_loss.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    resnet50.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valLoader:\n",
    "            images = images.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "            outputs = resnet50(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    avg_val_loss = np.mean(val_loss)\n",
    "    epoch_val_loss.append(avg_val_loss)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Val Loss: {np.mean(avg_val_loss):.4f}')\n",
    "\n",
    "    # Check for ES\n",
    "    if es(resnet50, avg_val_loss):\n",
    "        pbar.set_description(f\"Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, {es.status}\")\n",
    "        break\n",
    "    else:\n",
    "        pbar.set_description(f\"Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "create_graph(epoch_train_loss, epoch_val_loss)\n",
    "import tqdm\n",
    "\n",
    "es = EarlyStopping(patience=20, min_delta=0, restore_best_weights=True)\n",
    "\n",
    "\n",
    "epoch_train_loss = []\n",
    "epoch_val_loss = []\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = []\n",
    "    resnet50.train()\n",
    "    pbar = tqdm.tqdm(trainLoader, total=len(trainLoader))\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet50(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        pbar.set_description(f\"Epoch: [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = np.mean(train_loss)\n",
    "    epoch_train_loss.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    resnet50.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valLoader:\n",
    "            images = images.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "            outputs = resnet50(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    avg_val_loss = np.mean(val_loss)\n",
    "    epoch_val_loss.append(avg_val_loss)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Val Loss: {np.mean(avg_val_loss):.4f}')\n",
    "\n",
    "    # Check for ES\n",
    "    if es(resnet50, avg_val_loss):\n",
    "        pbar.set_description(f\"Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, {es.status}\")\n",
    "        break\n",
    "    else:\n",
    "        pbar.set_description(f\"Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "create_graph(epoch_train_loss, epoch_val_loss)\n",
    "\n",
    "if run_on_local == 1:\n",
    "    save_path = './'\n",
    "else:\n",
    "    save_path = '/content/drive/MyDrive/ML_final/'\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': resnet50.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            }, save_path + '200_resnet50.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#InceptionV3\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=45),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "#dataset = torchvision.datasets.ImageFolder(root='/content/data/CUB_200_2011/images', transform=transform\n",
    "dataset = torchvision.datasets.ImageFolder(root='./content/data/CUB_200_2011/images', transform=transform)\n",
    "# Split to 80%/10%/10%\n",
    "\n",
    "train_split = int(len(dataset) * .8)\n",
    "val_split = int(len(dataset) * .1)\n",
    "test_split = int(len(dataset) * .1)\n",
    "\n",
    "#Make sure splits match 1:1 with dataset\n",
    "total = train_split + val_split + test_split\n",
    "if total < len(dataset):\n",
    "    train_split = train_split + (len(dataset) - total)\n",
    "\n",
    "\n",
    "\n",
    "print(dataset, train_split, val_split, test_split, train_split + val_split + test_split )\n",
    "train, val, test = torch.utils.data.random_split(dataset, [train_split, val_split, test_split])\n",
    "\n",
    "num_workers=2\n",
    "batch_size=16\n",
    "\n",
    "trainLoader = DataLoader(train , batch_size=batch_size,\n",
    "                                           num_workers=num_workers,  shuffle=True)\n",
    "valLoader = DataLoader(val, batch_size=batch_size,\n",
    "                                          num_workers=num_workers )\n",
    "testLoader = DataLoader(test, batch_size=batch_size,\n",
    "                                          num_workers=num_workers)\n",
    "\n",
    "import torchvision.models as models\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "weightz = models.get_model_weights(\"inception_v3\")\n",
    "weightz = weightz.DEFAULT\n",
    "print(weightz)\n",
    "\n",
    "inception = models.inception_v3(weights=weightz)\n",
    "\n",
    "for param in inception.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "print(\"classes:\", num_classes)\n",
    "\n",
    "num_features = inception.AuxLogits.fc.in_features\n",
    "inception.AuxLogits.fc = torch.nn.Linear(num_features, num_classes)\n",
    "\n",
    "num_features = inception.fc.in_features\n",
    "inception.fc = torch.nn.Linear(num_features, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "params_to_update = []\n",
    "for param in inception.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Create an object of the model\n",
    "inception = inception.to(\"cuda\")\n",
    "\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.RMSprop(inception.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "import tqdm\n",
    "\n",
    "es = EarlyStopping(patience=20, min_delta=0, restore_best_weights=True)\n",
    "\n",
    "epoch_train_loss = []\n",
    "epoch_val_loss = []\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = []\n",
    "    #scheduler.step()\n",
    "    inception.train()\n",
    "    pbar = tqdm.tqdm(trainLoader, total=len(trainLoader))\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, aux_outputs = inception(images)\n",
    "        loss1 = loss_function(outputs, labels)\n",
    "        loss2 = loss_function(aux_outputs, labels)\n",
    "        loss = loss1 + 0.4*loss2\n",
    "\n",
    "        # loss = loss_function(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        pbar.set_description(f\"Epoch: [{epoch + 1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = np.mean(train_loss)\n",
    "    epoch_train_loss.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    inception.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valLoader:\n",
    "            images = images.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "            outputs = inception(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    avg_val_loss = np.mean(val_loss)\n",
    "    epoch_val_loss.append(avg_val_loss)\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Val Loss: {np.mean(avg_val_loss):.4f}')\n",
    "\n",
    "    # Check for ES\n",
    "    if es(inception, avg_val_loss):\n",
    "        pbar.set_description(f\"Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, {es.status}\")\n",
    "        break\n",
    "    else:\n",
    "        pbar.set_description(f\"Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "create_graph(epoch_train_loss, epoch_val_loss)\n",
    "\n",
    "#Test the model\n",
    "y_test = []\n",
    "y_test_predict = []\n",
    "\n",
    "inception.eval()\n",
    "for images, labels in testLoader:\n",
    "    images = images.to(\"cuda\")\n",
    "    labels = labels.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = inception(images).cpu()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_test_predict.extend(predicted.numpy())\n",
    "        y_test.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_test_predict))\n",
    "\n",
    "if run_on_local == 1:\n",
    "    save_path = './'\n",
    "else:\n",
    "    save_path = '/content/drive/MyDrive/ML_final/'\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': inception.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            }, save_path + '200_inception.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
